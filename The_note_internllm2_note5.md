# 书生·浦语大模型学习第五节学习笔记
## 笔记
1. 大模型部署面临的挑战
- 计算量巨大
![image](https://github.com/PURE281/my_dream/assets/93171238/73f83374-5bae-496b-85b4-6a5e22dfb42a)
- 内存开销巨大
![image](https://github.com/PURE281/my_dream/assets/93171238/ebdc9db9-c7da-41ba-a4ba-9a91a0c421d4)
- 访存瓶颈
这个比较有意思
显卡的计算能力是足够的，但是访存能力十分有限，因此导致了显卡在数据交换上花费了大量的时间，只有极少的时间在进行实际的计算上
虽然可以通过batch-size的设置增加访存量，但是性价比太低
![image](https://github.com/PURE281/my_dream/assets/93171238/afc14864-be28-4db7-9aed-ac180aa1e95d)
- 优化方案
-- 模型剪枝
  移除模型中不必要或多余的组件，比如参数，让模型更加搞笑。通过对模型中贡献有限的冗余参数进行剪枝，在保证性能最低下降的同时，减少存储需求， 提高计算效率
  - 非结构化剪枝
    移除个别参数，不考虑整体网络结构。将低于阈值的参数置零的方式对个别权重或神经元进行处理
  - 结构化剪枝
    根据预定义规则移除链接或分层结构，同时保持整体网络结构。一次性地针对整组权重，优势在于降低模型复杂性和内存使用，同时保证整体的LLM结构完整
  ![image](https://github.com/PURE281/my_dream/assets/93171238/7ebfc819-c9aa-488e-8f20-c3489ada3eeb)

-- 知识蒸馏
  通过引导轻量化的学生模型“模仿”性能更好，结构更复杂的教师模型，在不改变学生模型结构的情况下提高性能
  上下文学习，思维链，指令跟随
  ![image](https://github.com/PURE281/my_dream/assets/93171238/baba9d15-21f4-41e8-be18-08d05fa00694)

-- 量化
  量化技术将传统的表示方法中的浮点数转换为证书或其他离散形式，以减轻深度学习模型的存储和计算负担
  量化感知训练、量化感知微调、训练后量化
  知识点：由于调用大模型的过程是一种访存密集型的过程，因此量化的过程虽然增加了量化和反量化两个计算过程，但通过量化的操作减轻了内存负担，最终还是实现了提高性能的目的。也就是说虽然增加了计算负担，但由于硬件上的计算能力是过剩的，因此增加的计算对整体的性能影响可以忽略不计；但是对访存而言，减轻的内存带来的性能提升是很大的，因此综合下来，量化的方式是能有效提升性能的
![image](https://github.com/PURE281/my_dream/assets/93171238/ba7a83a4-64ca-44aa-aa05-c7dc80b5ae7f)

2. LMDeploy部署
LMDeploy简介
涵盖了LLM任务的全套轻量化、部署和服务解决方案。核心功能包括高效推理、可靠量化、便捷服务和有状态推理
![image](https://github.com/PURE281/my_dream/assets/93171238/740e048c-0ea1-43ab-a987-092422462d1b)

## 作业
### 基础作业
1. 配置 LMDeploy 运行环境


2. 以命令行方式与 InternLM2-Chat-1.8B 模型对话
